{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhZpDPU4GfOG7+UL5vM1VS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"VJWbtw0Asyjj","executionInfo":{"status":"error","timestamp":1710912315734,"user_tz":-360,"elapsed":6507,"user":{"displayName":"Emon Karmaker","userId":"09813692096123274517"}},"outputId":"5ed9ece3-0c86-4ae8-99ca-bc5f83c31a80"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'util_functions'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1159033637d9>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mshutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocessing_Dataset1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_eval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util_functions'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# -*- coding: utf-8 -*-\n","import torch\n","import numpy as np\n","import sys, copy, math, time, pdb, warnings, traceback\n","import pickle\n","import scipy.io as sio\n","import scipy.sparse as ssp\n","import os.path\n","import random\n","import argparse\n","from shutil import copy, rmtree, copytree\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from util_functions import *\n","from preprocessing_Dataset1 import *\n","from train_eval import *\n","from models import *\n","from torch_geometric.data import Data, Dataset\n","import traceback\n","import warnings\n","import sys\n","import xlwt\n","from torchsummary import summary\n","import gc\n","from sklearn.metrics import precision_recall_curve,roc_curve,roc_auc_score,f1_score,precision_score,recall_score,auc\n","\n","\n","\n","if __name__ == '__main__':\n","\n","    # Arguments\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--no-train', action='store_true', default=False)\n","    parser.add_argument('--dataset', help='dataset name')\n","\n","    parser.add_argument('--use-features', action='store_true', default=False)\n","\n","    args = parser.parse_args()\n","\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(2341)\n","    seed=2341\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    torch.backends.cudnn.benchmark=False\n","    torch.backends.cudnn.deterministic=True\n","    hop = 1\n","\n","    if not args.no_train:\n","        #Construct model\n","        print('training.....')\n","        data_combo = (args.dataset, '', '')\n","        u_features, v_features, net, labels, u_indices, v_indices, num_list = load_data(args.dataset)\n","        print('preprocessing end.')\n","        adj=torch.tensor(net)\n","        if args.use_features:\n","            n_features = u_features.shape[1] + v_features.shape[1]\n","        else:\n","            u_features, v_features = None, None\n","            n_features = 0\n","        all_indices = (u_indices, v_indices)\n","        print('begin constructing all_graphs')\n","        all_graphs = extracting_subgraphs(net, all_indices, labels,hop, u_features,v_features,hop*2+1)\n","        mydataset = MyDataset(all_graphs, root='data/{}{}/{}/train'.format(*data_combo))\n","        print('constructing all_graphs end.')\n","\n","        sum=0\n","        all_results=[]\n","        max_f1=0\n","\n","        for count in range(1):\n","            model = gGATLDA(515, side_features=args.use_features, n_side_features=515)\n","            #model=model.cuda()\n","            print('########',count,' training.'+'#########')\n","\n","\t        #K-fold cross-validation\n","            K=5\n","            all_f1_mean,all_f1_std=0,0\n","            all_accuracy_mean,all_accuracy_std=0,0\n","            all_recall_mean,all_recall_std=0,0\n","            all_precision_mean,all_precision_std=0,0\n","            all_auc_mean,all_auc_std=0,0\n","            all_aupr_mean,all_aupr_std=0,0\n","            truth=[]\n","            predict=[]\n","            f1_s=[]\n","            accuracy_s=[]\n","            recall_s=[]\n","            precision_s=[]\n","            auc_s=[]\n","            aupr_s=[]\n","            max=0\n","            for i in range(K):\n","                print('*'*25,i+1,'*'*25)\n","\n","                train_graphs,test_graphs=get_k_fold_data(K,i,mydataset)\n","                test_auc,f1,accuracy,recall,precision,auc,aupr,one_truth,one_predict=train_multiple_epochs(train_graphs,test_graphs, model, adj)\n","                truth.extend(one_truth)\n","                predict.extend(one_predict)\n","                f1_s.append(f1)\n","                accuracy_s.append(accuracy)\n","                recall_s.append(recall)\n","                precision_s.append(precision)\n","                auc_s.append(auc)\n","                aupr_s.append(aupr)\n","\n","\n","            print('#'*10,'Final k-fold cross validation results','#'*10)\n","            print('The %d-fold CV auc: %f +/- %f' %(i,np.mean(auc_s),np.std(auc_s)))\n","            print('The %d-fold CV aupr: %f +/- %f' %(i,np.mean(aupr_s),np.std(aupr_s)))\n","            print('The %d-fold CV f1-score: %f +/- %f' %(i,np.mean(f1_s),np.std(f1_s)))\n","            print('The %d-fold CV recall: %f +/- %f' %(i,np.mean(recall_s),np.std(recall_s)))\n","            print('The %d-fold CV accuracy: %f +/- %f' %(i,np.mean(accuracy_s),np.std(accuracy_s)))\n","            print('The %d-fold CV precision: %f +/- %f' %(i,np.mean(precision_s),np.std(precision_s)))\n","            all_f1_mean=all_f1_mean+np.mean(f1_s)\n","            all_f1_std=all_f1_std+np.std(f1_s)\n","\n","            all_recall_mean=all_recall_mean+np.mean(recall_s)\n","            all_recall_std=all_recall_std+np.std(recall_s)\n","\n","            all_accuracy_mean=all_accuracy_mean+np.mean(accuracy_s)\n","            all_accuracy_std=all_accuracy_std+np.std(accuracy_s)\n","\n","            all_precision_mean=all_precision_mean+np.mean(precision_s)\n","            all_precision_std=all_precision_std+np.std(precision_s)\n","\n","            all_auc_mean=all_auc_mean+np.mean(auc_s)\n","            all_auc_std=all_auc_std+np.std(auc_s)\n","\n","            all_aupr_mean=all_aupr_mean+np.mean(aupr_s)\n","            all_aupr_std=all_aupr_std+np.std(aupr_s)\n","\n","            truth_predict=[truth,predict]\n","            all_results.append(truth_predict)\n","\n","\n","        np.save('results/log_truth_Dataset1_CV1.npy',np.array(truth))\n","        np.save('results/log_predict_Dataset1_CV1.npy',np.array(predict))\n","        torch.save(model,'model.pth')\n","\n","\n","\n","    print(\"All end...\")\n","\n","\n","\n"]},{"cell_type":"code","source":["import torch\n","import math\n","import torch.nn as nn\n","from torch_geometric.nn import MessagePassing,GATConv\n","import torch.nn.functional as F\n","from torch.nn import Linear, Conv1d,AdaptiveMaxPool2d\n","from torch_geometric.nn import GCNConv, RGCNConv, global_sort_pool, global_add_pool, global_max_pool,global_mean_pool\n","from torch_geometric.utils import dropout_adj\n","from util_functions import *\n","import pdb\n","import time\n","from torch.autograd import Variable\n","from ranger import Ranger\n","from ranger import RangerVA\n","from ranger import RangerQH\n","\n","\n","\n","class gGATLDA(torch.nn.Module):\n","    # The gGATLDA model use GCN layer + GAT layer\n","    def __init__(self, in_features, gconv=GATConv, latent_dim=[16, 16, 16, 16], side_features=False, n_side_features=0):\n","        super(gGATLDA, self).__init__()\n","        self.convs = torch.nn.ModuleList()\n","        self.conv1=GCNConv((in_features), 16)\n","        self.convs.append(gconv(16, latent_dim[0],heads=8,dropout=0.2))\n","        self.convs.append(gconv(16*8, 16,heads=8,dropout=0.2))\n","        self.convs.append(gconv(16*8, 16,heads=8,dropout=0.2))\n","        self.convs.append(gconv(16*8, 2,heads=1,dropout=0.2))\n","        self.conv2=gconv(in_channels=2*8,out_channels=2,dropout=0.2,heads=1,concat=True)\n","        self.lin1 = Linear(3*sum(latent_dim), 8)\n","        self.lin2 = Linear(2*4*16, 8)\n","\n","    def forward(self, data):\n","        start = time.time()\n","        x, edge_index, batch = data.x, data.edge_index, data.batch\n","        concat_states=[]\n","        x = F.elu(self.conv1(x, edge_index))\n","        for conv in self.convs:\n","            x = F.elu(conv(x, edge_index))\n","            concat_states.append(x)\n","        concat_states = torch.cat(concat_states, 1)\n","        users = data.x[:, 0] == 1\n","        items = data.x[:, 1] == 1\n","        x = torch.cat([x[users], x[items]], 1)\n","        return F.log_softmax(x, dim=1)\n","\n","    def predict(self,data):\n","        out=self.forward(data)\n","        return out\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"yq0aXnFKtEwr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","import scipy.sparse as sp\n","import scipy.io as scio\n","import pickle as pkl\n","import os\n","import h5py\n","import pandas as pd\n","import random\n","import pdb\n","import math\n","from random import randint,sample\n","from sklearn.model_selection import KFold\n","\n","def load_data(dataset):\n","    print(\"Loading lncRNAdisease dataset\")\n","    path_dataset = 'raw_data/' + dataset + '/training_test_dataset.mat'\n","    data=scio.loadmat(path_dataset)\n","    net=data['interMatrix']\n","\n","    #lncRNA features and disease features\n","    u_features=data['lncSim']\n","    disSim_path='raw_data/' + dataset + '/disSim.xlsx'\n","    disSim_data=pd.read_excel(disSim_path,header=0)\n","    v_features=np.array(disSim_data)\n","\n","    num_list=[len(u_features)]\n","    num_list.append(len(v_features))\n","    temp=np.zeros((net.shape[0],net.shape[1]),int)\n","    u_features=np.hstack((u_features,net))\n","    v_features=np.hstack((net.T,v_features))\n","\n","    a=np.zeros((1,u_features.shape[0]+v_features.shape[0]),int)\n","    b=np.zeros((1,v_features.shape[0]+u_features.shape[0]),int)\n","    u_features=np.vstack((a,u_features))\n","    v_features=np.vstack((b,v_features))\n","\n","    num_lncRNAs=net.shape[0]\n","    num_diseases=net.shape[1]\n","\n","    row,col,_=sp.find(net)\n","    perm=random.sample(range(len(row)),len(row))\n","    row,col=row[perm],col[perm]\n","    sample_pos=(row,col)\n","    print(\"the number of all positive sample:\",len(sample_pos[0]))\n","\n","    print(\"sampling negative links for train and test\")\n","    sample_neg=([],[])\n","    net_flag=np.zeros((net.shape[0],net.shape[1]))\n","    X=np.ones((num_lncRNAs,num_diseases))\n","    net_neg=X-net\n","    row_neg,col_neg,_=sp.find(net_neg)\n","    perm_neg=random.sample(range(len(row_neg)),len(row))\n","    row_neg,col_neg=row_neg[perm_neg],col_neg[perm_neg]\n","    sample_neg=(row_neg,col_neg)\n","    sample_neg=list(sample_neg)\n","    print(\"the number of all negative sample:\", len(sample_neg[0]))\n","\n","    u_idx = np.hstack([sample_pos[0], sample_neg[0]])\n","    v_idx = np.hstack([sample_pos[1], sample_neg[1]])\n","    labels= np.hstack([[1]*len(sample_pos[0]), [0]*len(sample_neg[0])])\n","\n","    l1=np.zeros((1,net.shape[1]),int)\n","    print(l1.shape)\n","    net=np.vstack([l1,net])\n","    print(\"old net:\",net.shape)\n","    l2=np.zeros((net.shape[0],1),int)\n","    net=np.hstack([l2,net])\n","    print(\"new net:\",net.shape)\n","\n","    u_idx=u_idx+1\n","    v_idx=v_idx+1\n","\n","    return u_features, v_features, net, labels, u_idx, v_idx,num_list\n","\n","def load_predict_data(dataset):\n","    print(\"Loading lncRNAdisease dataset\")\n","    path_dataset = 'raw_data/' + dataset + '/training_test_dataset.mat'\n","    data=scio.loadmat(path_dataset)\n","    net=data['interMatrix']\n","    num_lncRNAs=net.shape[0]\n","    num_diseases=net.shape[1]\n","\n","    net_new=np.zeros((num_lncRNAs+1,num_diseases+1),dtype=np.int32)\n","    for i in range(1,num_lncRNAs+1):\n","        for j in range(1,num_diseases+1):\n","            net_new[i,j]=net[i-1,j-1]\n","    u_features=data['lncSim']\n","    disSim_path='raw_data/' + dataset + '/disSim.xlsx'\n","    disSim_data=pd.read_excel(disSim_path,header=0)\n","    v_features=np.array(disSim_data)\n","\n","    num_list=[len(u_features)]\n","    num_list.append(len(v_features))\n","    temp=np.zeros((net.shape[0],net.shape[1]),int)\n","    u_features=np.hstack((u_features,net))\n","    v_features=np.hstack((net.T,v_features))\n","    a=np.zeros((1,u_features.shape[0]+v_features.shape[0]),int)\n","    b=np.zeros((1,v_features.shape[0]+u_features.shape[0]),int)\n","    u_features=np.vstack((a,u_features))\n","    v_features=np.vstack((b,v_features))\n","\n","    #loading miRNA_name and disease_name\n","    lncRNA_name=[]\n","    disease_name=[]\n","    disease_name.append([])\n","    lncRNA_name.append([])\n","    f=open('raw_data/' + dataset+'/lncRNA_Name.txt','r')\n","    while True:\n","        line=f.readline()\n","        if not line:\n","            break\n","        lncRNA_name.append(line)\n","    f.close()\n","    f=open('raw_data/' + dataset+'/disease_Name.txt','r')\n","    while True:\n","        line=f.readline()\n","        if not line:\n","            break\n","        disease_name.append(line)\n","    f.close()\n","    print(\"lncRNA_name:\",len(lncRNA_name))\n","    case_disease='renal carcinoma\\n'\n","    if case_disease in disease_name:\n","        idx=disease_name.index(case_disease)\n","\n","    u_idx,v_idx, labels=[],[],[]\n","    list=[]\n","    for i in range(1,net_new.shape[0]):\n","        if net_new[i][idx]==0:\n","            list.append([i,idx,net_new[i][idx]])\n","\n","    for i in range(len(list)):\n","        u_idx.append(list[i][0])\n","        v_idx.append(list[i][1])\n","        labels.append(list[i][2])\n","    class_values=np.array([0,1],dtype=float)\n","\n","    return u_features, v_features, net_new, labels, u_idx, v_idx, class_values,lncRNA_name,disease_name\n","\n","\n"],"metadata":{"id":"wQy9vC2zUWU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import os\n","import math\n","import multiprocessing as mp\n","import numpy as np\n","import networkx as nx\n","import torch\n","import torch.nn.functional as F\n","from torch import tensor\n","from torch.optim import Adam\n","from sklearn.model_selection import StratifiedKFold\n","from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n","from tqdm import tqdm\n","import pdb\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","from util_functions import PyGGraph_to_nx\n","from sklearn.metrics import roc_auc_score,accuracy_score\n","from sklearn import metrics\n","import random\n","from ranger import Ranger\n","from ranger import RangerVA\n","from ranger import RangerQH\n","from torch.optim import *\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def get_k_fold_data(k,i,data):\n","    assert k>1\n","    data_pos=data[0:621]\n","    data_neg=data[621:1242]\n","\n","    start=int(i*621//k)\n","    end=int((i+1)*621//k)\n","\n","    data_train, data_valid=None, None\n","    data_valid_pos, data_valid_neg=None, None\n","    data_train_pos, data_train_neg=None, None\n","\n","    data_valid_pos=data_pos[start:end]\n","    data_train_pos=data_pos[0:start]+data_pos[end:621]\n","    data_valid_neg=data_neg[start:end]\n","    data_train_neg=data_neg[0:start]+data_neg[end:621]\n","    data_train=data_train_pos+data_train_neg\n","    data_valid=data_valid_pos+data_valid_neg\n","    return data_train,data_valid\n","\n","\n","\n","def train_multiple_epochs(train_graphs, test_graphs, model,  adj):\n","    train_loss,test_loss=[],[]\n","\n","    print(\"starting train...\")\n","    LR=0.01\n","    batch_size=64\n","    epochs=50\n","    train_loader = DataLoader(train_graphs, batch_size, shuffle=True, num_workers=0)\n","    test_loader = DataLoader(test_graphs, batch_size, shuffle=True, num_workers=0)\n","    optimizer = Ranger(model.parameters(), lr=0.001, weight_decay=0)\n","    start_epoch = 1\n","    pbar = tqdm(range(start_epoch, epochs + start_epoch))\n","    count=0\n","\n","    for epoch in pbar:\n","        total_loss=0\n","\n","\n","        model.train()\n","        for data in train_loader:\n","            optimizer.zero_grad()\n","            #data=data.cuda()\n","            out = model(data)\n","            loss=F.cross_entropy(out, data.y.view(-1).long())\n","            loss.backward()\n","            total_loss+=loss.item()*num_graphs(data)\n","            optimizer.step()\n","        train_loss=total_loss/len(train_loader.dataset)\n","        train_auc=evaluate(model,train_loader,1)\n","        print('\\n Epoch: {:03d}, Loss: {:.5f}, Train Auc: {:.5f}'.format(epoch, train_loss, train_auc))\n","\n","\n","    test_auc,one_pred_result=evaluate(model,test_loader,2)\n","    truth=one_pred_result[1]\n","    predict=one_pred_result[0]\n","    vmax=max(predict)\n","    vmin=min(predict)\n","\n","    alpha=0.8\n","    predict_f1=[0 for x in range(len(predict))]\n","    for p in range(len(predict)):\n","        predict_f1[p]=(predict[p]-vmin)/(vmax-vmin)\n","    predict_f1=[int(item>alpha) for item in predict_f1]\n","\n","    f1=metrics.f1_score(truth,predict_f1)\n","    accuracy=metrics.accuracy_score(truth,predict_f1)\n","    recall=metrics.recall_score(truth,predict_f1)\n","    precision=metrics.precision_score(truth,predict_f1)\n","    fpr,tpr, thresholds1=metrics.roc_curve(truth,predict,pos_label=1)\n","    auc_score=metrics.auc(fpr,tpr)\n","    p,r,thresholds2=metrics.precision_recall_curve(truth,predict,pos_label=1)\n","    aupr_score=metrics.auc(r,p)\n","    print('f1:',f1)\n","    print('accuracy:',accuracy)\n","    print('recall:',recall)\n","    print('precision:',precision)\n","    print('auc:',auc_score)\n","    print('aupr:',aupr_score)\n","    print('test_auc:',test_auc)\n","    return test_auc, f1,accuracy,recall,precision,auc_score,aupr_score,truth,predict\n","\n","def evaluate(model,loader,flag):\n","    one_pred_result=[]\n","    model.eval()\n","    predictions=torch.Tensor()\n","    labels=torch.Tensor()\n","    with torch.no_grad():\n","        for data in loader:\n","            data=data.to(device)\n","            pred=model(data)\n","            #predictions.append(pred[:,1].cpu().detach())\n","\n","            predictions=torch.cat((predictions,pred[:,1].detach()),0)\n","            labels=torch.cat((labels,data.y),0)\n","\n","    #labels=labels.cuda().data.cpu().numpy()\n","    #predictions=predictions.cuda().data.cpu().numpy()\n","    if flag==2:\n","        one_pred_result=np.vstack((predictions,labels))\n","\n","    fpr,tpr,_=metrics.roc_curve(labels,predictions,pos_label=1)\n","    auc=metrics.auc(fpr,tpr)\n","    if flag==1:\n","        return auc\n","    else:\n","        return auc,one_pred_result;\n","\n","\n","def num_graphs(data):\n","    if data.batch is not None:\n","        return data.num_graphs\n","    else:\n","        return data.x.size(0)\n","\n","\n","\n","\n"],"metadata":{"id":"zd-PZVz5Ubkz"},"execution_count":null,"outputs":[]}]}